# âš™ï¸ MODULE 2: Data Processing

This module focused on modern tools and techniques for processing data efficiently and at scale. It covered the full data journeyâ€”from ingestion to transformation and visualizationâ€”using both batch and streaming technologies.


## âœ… What I Learned

### ğŸ”„ Data Ingestion  
- Using **Apache NiFi** to automate and manage data flows

### ğŸ“¡ Change Data Capture (CDC)  
- Streaming database changes with **Redpanda** (Kafka-compatible)

### ğŸƒ NoSQL  
- Working with **MongoDB**: document models, collections, and basic queries

### ğŸ“¬ Kafka  
- Using **Confluent Kafka** for real-time data streaming  
- Producing and consuming messages, working with topics and schemas

### âš¡ PySpark  
- Distributed data processing with **PySpark**  
- RDDs, DataFrames, and transformations

### ğŸŒ APIs  
- Building and testing APIs with **Flask** and **Swagger**

### ğŸ› ï¸ DBT  
- Data modeling and transformations in the warehouse using **DBT**

### ğŸ“Š Tableau  
- Creating dashboards and visualizing data insights with **Tableau**


## ğŸ“‚ What You Can Find in This Folder

- ğŸ“¬ [**Kafka Assignment**](KAFKA) â€“ Main assignment using Confluent Kafka (Docker). A streaming pipeline using Kafka with a producer that sends movie data in JSON format to a topic. A consumer processes the data and filters it and sends it to a new topic. Additionally, KSQL is used to create a stream for real-time querying of the filtered data.
- âš¡ [**PySpark Assignment**](PYSPARK) â€“ A data analysis project multiple datasets, including data cleaning, joins, aggregations, and correlations. The final results were also loaded into a MySQL database as part of the pipeline.
